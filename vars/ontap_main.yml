---
# Role variables as per NetApp's prescriptive guidance
# This can be overridden by a var-file.yml at the command line
# User's input variables

#Name of the ONTAP Cluster
cluster_name: ontap

#Names of the Nodes in the ONTAP Cluster
nodes: 
 - ontap1
 - ontap2

#List the names of the nodes and all their ports that will be used (Do not include Cluster Ports)
#These portnames are used to remove them from default broadcast domain created during cluster setup
node_ports:
  - {node_name: "ontap1", ports: ["e0c","e0d","e0e","e0f"]}
  - {node_name: "ontap2", ports: ["e0c","e0d","e0e","e0f"]}

#List the names of the nodes and all their ports that will be used for data traffic
#These this should not include cluster and UTA ports used as fc target
node_data_ports:
  - {node_name: "ontap1", ports: ["e0c","e0d"]}
  - {node_name: "ontap2", ports: ["e0c","e0d"]}

#If your node data ports include any UTA/CNA ports, please list them below
#Do not include UTA ports used for FC target functionality
node_uta_ports:
  - {node_name: "ontap1", uta_ports: ["0e","0f"]}
  - {node_name: "ontap2", uta_ports: ["0e","0f"]}

#FCP Adapter Ports
node_fcp_ports:
  - 0e
  - 0f

# Network settings for the Service processor
sp:
  - {node: "ontap1", ip: "192.168.3.153", mask: "255.255.255.0", gateway: "192.168.3.1"}
  - {node: "ontap2", ip: "192.168.3.154", mask: "255.255.255.0", gateway: "192.168.3.1"}

#List the ONTAP Licenses for the different features that you need
ontap_license:
  - <License key for feature 1>
  - <License key for feature 2>
  - <License key for feature 3>

#Details of the Data Aggregates that need to be created
#If Aggregate creation takes longer, subsequent tasks of creating volumes may fail.
#There should be enough disks already zeroed in the cluster, otherwise aggregate create will zero the disks and will take long time
data_aggregates:
  - {aggr_name: aggr01_node01, diskcount: 5}
  - {aggr_name: aggr01_node02, diskcount: 5}

#Name of the Interface group to be created
ifgrp_name: a0a

ifgrp_mode: multimode_lacp

ontap_vlan_state: present

#Details of the NFS LIFs to be created
nfs_lifs:
  - {name: nfs_lif01, address: 192.168.203.151, netmask: 255.255.255.0, home_node: "ontap1" }
  - {name: nfs_lif02, address: 192.168.203.152, netmask: 255.255.255.0, home_node: "ontap2" }

#Details of the FCP LIFs to be created
fcp_lifs:
  - {name: fcp_lif01a, home_port: 0e, home_node: "ontap1"}
  - {name: fcp_lif01b, home_port: 0f, home_node: "ontap1"}
  - {name: fcp_lif02a, home_port: 0e, home_node: "ontap2"}
  - {name: fcp_lif02b, home_port: 0f, home_node: "ontap2"}

#FCP Adapter Port Speed
fcp_port_speed: "8"

#Client match for the export policy rule
client_match: 192.168.203.0/24

#Data Protocol to be used for export policy rules
data_protocol: nfs

#SVM Management LIF Details
infra_svm_specs:
  - {home_port: e0M, address: 192.168.3.155, netmask: 255.255.255.0, gateway: 192.168.3.1, lif_name: infra_svm_mgmt} #Update this line

#Infrastructure Volumes/ Datastores to be created
datastores:
  - {name: infra_datastore1, size: 500}
  - {name: infra_datastore2, size: 500}

#Details of the Boot Volume that will host the Boot LUNs for the Operating Systems
boot_volume:
  - {name: esxi_boot, size: 100}

#Details of the Boot LUNs on which the Operating System will be installed, add one entry for each LUN
first_boot_lun: 1
boot_lun_prefix: VM-Host-Infra-%02d
boot_lun_size: 15

#Operating System that will be installed on the Boot LUNs
infra_OS: vmware

# Set of default variables according to FlexPod best practices. Change only if required
port_mode: "cna"
port_type:  "target"

# Following two variables are used in a task to ensure auto revert for cluster management LIF is set to True.
cluster_mgmt_auto_revert: true

# Cluster management LIF already exists (pre-requisite), so note down name of cluster management LIF and enter below.
cluster_mgmt_interface: cluster_mgmt

#Name of SVM created for infrastructure data, i.e. ESXi Boot LUNs 
infrastructure_svm: infra2_svm

#Password for vsadmin user of Infrastructure SVM
vsadmin_password: NetApp!23

#List of aggregates for infra SVM
aggr_list: []

# Root volume for infrastructure svm
infra_root_volume: infra_root

#Create root mirror volumes for SVM
mirror_volumes:
  - {mirror_volume: rootvol_m01}
  - {mirror_volume: rootvol_m02}

#Job Schedule
job_schedule:
  - {job_name: 15min,job_minutes: 15}

#Details for configuring NetApp AutoSupport
autosupport_vars:
  mail_hosts: "mailhost"
  noteto: "admin@netapp.com"
